{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0teI28HTjS3"
      },
      "outputs": [],
      "source": [
        "# PySpark ile CSV'den Yalan Haber Tespiti - Tam Pipeline\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer\n",
        "from pyspark.ml.classification import LogisticRegression, NaiveBayes\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.sql.types import DoubleType\n",
        "from builtins import max\n",
        "\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. SPARK SESSION BAÅLATMA\n",
        "# ========================\n",
        "def initialize_spark():\n",
        "    \"\"\"Spark session baÅŸlatÄ±r - CSV iÃ§in optimize edilmiÅŸ\"\"\"\n",
        "\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"FakeNewsDetection_CSV\") \\\n",
        "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\") \\\n",
        "        .config(\"spark.driver.memory\", \"4g\") \\\n",
        "        .config(\"spark.executor.memory\", \"4g\") \\\n",
        "        .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
        "        .config(\"spark.network.timeout\", \"300s\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "    spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "    print(\"âœ… Spark Session baÅŸlatÄ±ldÄ±\")\n",
        "    print(f\"ğŸ“‹ Spark Version: {spark.version}\")\n",
        "\n",
        "    return spark\n"
      ],
      "metadata": {
        "id": "oGWNzR_qT4iO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ========================\n",
        "# 2. CSV VERÄ° YÃœKLEME\n",
        "# ========================\n",
        "def load_csv_data(spark, csv_path):\n",
        "    \"\"\"\n",
        "    CSV dosyasÄ±ndan veri yÃ¼kler\n",
        "\n",
        "    Args:\n",
        "        spark: SparkSession\n",
        "        csv_path: CSV dosyasÄ±nÄ±n yolu\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: text ve label kolonlarÄ± iÃ§eren DataFrame\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"ğŸ“– CSV dosyasÄ± okunuyor: {csv_path}\")\n",
        "\n",
        "    try:\n",
        "        # CSV'yi Spark DataFrame olarak oku\n",
        "        df = spark.read \\\n",
        "            .option(\"header\", \"true\") \\\n",
        "            .option(\"inferSchema\", \"true\") \\\n",
        "            .option(\"encoding\", \"UTF-8\") \\\n",
        "            .option(\"multiline\", \"true\") \\\n",
        "            .option(\"escape\", '\"') \\\n",
        "            .csv(csv_path)\n",
        "\n",
        "        # Kolon isimlerini kontrol et ve standartlaÅŸtÄ±r\n",
        "        columns = df.columns\n",
        "        print(f\"ğŸ“‹ Bulunan kolonlar: {columns}\")\n",
        "\n",
        "        # Kolon isimlerini temizle (boÅŸluk, Ã¶zel karakter kaldÄ±r)\n",
        "        for col_name in columns:\n",
        "            clean_name = col_name.strip().lower()\n",
        "            if col_name != clean_name:\n",
        "                df = df.withColumnRenamed(col_name, clean_name)\n",
        "\n",
        "        # Kolon isimlerini standart hale getir\n",
        "        columns = df.columns\n",
        "        text_col = None\n",
        "        label_col = None\n",
        "\n",
        "        # Metin kolonu bulma\n",
        "        for column in columns:\n",
        "            if any(keyword in column.lower() for keyword in ['metin', 'text', ...]):\n",
        "                text_col = column\n",
        "                break\n",
        "\n",
        "\n",
        "        # Etiket kolonu bulma\n",
        "        for column in columns:\n",
        "            if any(keyword in column.lower() for keyword in ['etiket', 'label']):\n",
        "                label_col = column\n",
        "                break\n",
        "\n",
        "        if not text_col or not label_col:\n",
        "            print(f\"âš ï¸ Uygun kolonlar bulunamadÄ±. Mevcut kolonlar: {columns}\")\n",
        "            print(\"ğŸ’¡ Ä°lk kolonu metin, ikinci kolonu etiket olarak kullanacaÄŸÄ±m.\")\n",
        "            text_col = columns[0]\n",
        "            label_col = columns[1]\n",
        "\n",
        "        print(f\"ğŸ“ Metin kolonu: {text_col}\")\n",
        "        print(f\"ğŸ·ï¸ Etiket kolonu: {label_col}\")\n",
        "\n",
        "        # KolonlarÄ± yeniden adlandÄ±r\n",
        "        df = df.select(\n",
        "            col(text_col).alias(\"text\"),\n",
        "            col(label_col).alias(\"label_raw\")\n",
        "        )\n",
        "\n",
        "        # Null deÄŸerleri filtrele\n",
        "        df = df.filter(col(\"text\").isNotNull() & col(\"label_raw\").isNotNull())\n",
        "        df = df.filter((col(\"text\") != \"\") & (length(col(\"text\")) > 10))\n",
        "\n",
        "        # Etiketleri kontrol et ve dÃ¶nÃ¼ÅŸtÃ¼r\n",
        "        unique_labels = df.select(\"label_raw\").distinct().collect()\n",
        "        print(f\"ğŸ“Š Unique etiketler: {[row.label_raw for row in unique_labels]}\")\n",
        "\n",
        "        # String etiketleri sayÄ±sal deÄŸerlere Ã§evir\n",
        "        df = convert_labels_to_numeric(df)\n",
        "\n",
        "        # Veri istatistikleri\n",
        "        total_count = df.count()\n",
        "        label_counts = df.groupBy(\"label\").count().collect()\n",
        "\n",
        "        print(f\"âœ… Toplam {total_count} haber yÃ¼klendi\")\n",
        "        for row in label_counts:\n",
        "            label_name = \"DoÄŸru Haber\" if row.label == 1 else \"Yalan Haber\"\n",
        "            print(f\"   - {label_name}: {row.count}\")\n",
        "\n",
        "        # DataFrame'i optimize et\n",
        "        df = df.coalesce(4).cache()\n",
        "\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ CSV okuma hatasÄ±: {e}\")\n",
        "        raise\n",
        "\n",
        "def convert_labels_to_numeric(df):\n",
        "    \"\"\"Etiketleri sayÄ±sal deÄŸerlere Ã§evirir\"\"\"\n",
        "\n",
        "    # Ã–nce etiket tÃ¼rÃ¼nÃ¼ kontrol et\n",
        "    sample_labels = df.select(\"label_raw\").limit(10).collect()\n",
        "    sample_values = [row.label_raw for row in sample_labels]\n",
        "\n",
        "    print(f\"ğŸ” Ã–rnek etiket deÄŸerleri: {sample_values}\")\n",
        "\n",
        "    # EÄŸer zaten sayÄ±sal ise\n",
        "    if all(isinstance(val, (int, float)) for val in sample_values if val is not None):\n",
        "        print(\"ğŸ“Š Etiketler zaten sayÄ±sal\")\n",
        "        # 0 ve 1'e normalize et\n",
        "        df = df.withColumn(\"label\",\n",
        "                          when(col(\"label_raw\") > 0, 1).otherwise(0))\n",
        "    else:\n",
        "        # String etiketleri dÃ¶nÃ¼ÅŸtÃ¼r\n",
        "        print(\"ğŸ”„ String etiketler sayÄ±sal deÄŸerlere Ã§evriliyor\")\n",
        "\n",
        "        # YaygÄ±n etiket tÃ¼rleri iÃ§in mapping\n",
        "        df = df.withColumn(\"label\",\n",
        "            when(lower(col(\"label_raw\")).isin([\"true\", \"doÄŸru\", \"gerÃ§ek\", \"real\", \"1\", \"dogru\"]), 1)\n",
        "            .when(lower(col(\"label_raw\")).isin([\"false\", \"yalan\", \"sahte\", \"fake\", \"0\"]), 0)\n",
        "            .otherwise(\n",
        "                when(col(\"label_raw\").cast(\"int\").isNotNull(),\n",
        "                     when(col(\"label_raw\").cast(\"int\") > 0, 1).otherwise(0))\n",
        "                .otherwise(0)  # Bilinmeyen deÄŸerleri 0 yap\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return df.drop(\"label_raw\")\n"
      ],
      "metadata": {
        "id": "BsVQgUiRUEZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# 3. VERÄ° KEÅFÄ° VE ANALÄ°ZÄ°\n",
        "# ========================\n",
        "def explore_data(df):\n",
        "    \"\"\"Veri keÅŸfi ve analizi\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"ğŸ“Š VERÄ° KEÅFÄ° VE ANALÄ°ZÄ°\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Temel istatistikler\n",
        "    print(\"ğŸ“‹ Temel Ä°statistikler:\")\n",
        "    df.describe().show()\n",
        "\n",
        "    # Etiket daÄŸÄ±lÄ±mÄ±\n",
        "    print(\"ğŸ“Š Etiket DaÄŸÄ±lÄ±mÄ±:\")\n",
        "    df.groupBy(\"label\").count().orderBy(\"label\").show()\n",
        "\n",
        "    # Metin uzunluk istatistikleri\n",
        "    print(\"ğŸ“ Metin Uzunluk Ä°statistikleri:\")\n",
        "    df.select(\n",
        "        length(col(\"text\")).alias(\"text_length\")\n",
        "    ).describe().show()\n",
        "\n",
        "    # Ã–rnek metinler\n",
        "    print(\"ğŸ“ Ã–rnek Metinler:\")\n",
        "    sample_texts = df.select(\"text\", \"label\").limit(3).collect()\n",
        "    for i, row in enumerate(sample_texts, 1):\n",
        "        label_name = \"DoÄŸru\" if row.label == 1 else \"Yalan\"\n",
        "        print(f\"\\n{i}. {label_name} Haber:\")\n",
        "        print(f\"   {row.text[:200]}...\")\n",
        "\n",
        "    # Null deÄŸer kontrolÃ¼\n",
        "    print(\"ğŸ” Null DeÄŸer KontrolÃ¼:\")\n",
        "    null_counts = df.select(\n",
        "        sum(col(\"text\").isNull().cast(\"int\")).alias(\"text_nulls\"),\n",
        "        sum(col(\"label\").isNull().cast(\"int\")).alias(\"label_nulls\")\n",
        "    ).collect()[0]\n",
        "\n",
        "    print(f\"   Text null: {null_counts.text_nulls}\")\n",
        "    print(f\"   Label null: {null_counts.label_nulls}\")\n"
      ],
      "metadata": {
        "id": "YvQSWgpaZFjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# 4. METÄ°N Ã–N Ä°ÅLEME PÄ°PELÄ°NE\n",
        "# ========================\n",
        "def create_preprocessing_pipeline():\n",
        "    \"\"\"Metin Ã¶niÅŸleme pipeline'Ä± oluÅŸturur\"\"\"\n",
        "\n",
        "    print(\"ğŸ”§ Metin Ã¶niÅŸleme pipeline'Ä± oluÅŸturuluyor...\")\n",
        "\n",
        "    # 1. Tokenizasyon\n",
        "    tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "\n",
        "    # 2. Stop words kaldÄ±rma (TÃ¼rkÃ§e + Ä°ngilizce)\n",
        "    turkish_stopwords = [\n",
        "        \"ve\", \"ile\", \"bu\", \"ÅŸu\", \"o\", \"da\", \"de\", \"ta\", \"te\", \"ki\", \"mi\", \"mu\", \"mÃ¼\",\n",
        "        \"iÃ§in\", \"gibi\", \"kadar\", \"sonra\", \"Ã¶nce\", \"Ã¼zere\", \"gÃ¶re\", \"karÅŸÄ±\", \"raÄŸmen\",\n",
        "        \"bir\", \"iki\", \"Ã¼Ã§\", \"dÃ¶rt\", \"beÅŸ\", \"altÄ±\", \"yedi\", \"sekiz\", \"dokuz\", \"on\",\n",
        "        \"ben\", \"sen\", \"biz\", \"siz\", \"onlar\", \"benim\", \"senin\", \"bizim\", \"sizin\",\n",
        "        \"bunun\", \"ÅŸunun\", \"onun\", \"bunlar\", \"ÅŸunlar\", \"olan\", \"oldu\", \"olur\", \"olsa\",\n",
        "        \"daha\", \"Ã§ok\", \"az\", \"en\", \"hem\", \"ya\", \"veya\", \"ama\", \"fakat\", \"ancak\",\n",
        "        \"her\", \"hiÃ§\", \"kendi\", \"tÃ¼m\", \"bÃ¼tÃ¼n\", \"hangi\", \"nasÄ±l\", \"neden\", \"niÃ§in\"\n",
        "    ]\n",
        "\n",
        "    english_stopwords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
        "    all_stopwords = list(set(turkish_stopwords + english_stopwords))\n",
        "\n",
        "    stop_remover = StopWordsRemover(\n",
        "        inputCol=\"words\",\n",
        "        outputCol=\"filtered_words\",\n",
        "        stopWords=all_stopwords\n",
        "    )\n",
        "\n",
        "    # 3. TF-IDF\n",
        "    hashing_tf = HashingTF(\n",
        "        inputCol=\"filtered_words\",\n",
        "        outputCol=\"raw_features\",\n",
        "        numFeatures=20000  # Ã–zellik sayÄ±sÄ±nÄ± artÄ±rdÄ±k\n",
        "    )\n",
        "\n",
        "    idf = IDF(\n",
        "        inputCol=\"raw_features\",\n",
        "        outputCol=\"features\",\n",
        "        minDocFreq=2  # En az 2 dokÃ¼manda geÃ§en terimleri al\n",
        "    )\n",
        "\n",
        "    # Pipeline oluÅŸtur\n",
        "    preprocessing_pipeline = Pipeline(stages=[\n",
        "        tokenizer,\n",
        "        stop_remover,\n",
        "        hashing_tf,\n",
        "        idf\n",
        "    ])\n",
        "\n",
        "    print(\"âœ… Metin Ã¶niÅŸleme pipeline'Ä± oluÅŸturuldu\")\n",
        "    return preprocessing_pipeline\n"
      ],
      "metadata": {
        "id": "2j67fI3uZIlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ========================\n",
        "# 5. MODEL EÄÄ°TÄ°MÄ°\n",
        "# ========================\n",
        "def train_logistic_regression(train_df):\n",
        "    \"\"\"Logistic Regression modeli eÄŸitir\"\"\"\n",
        "    lr = LogisticRegression(\n",
        "        featuresCol=\"features\",\n",
        "        labelCol=\"label\",\n",
        "        maxIter=100,\n",
        "        regParam=0.1,\n",
        "        elasticNetParam=0.8,\n",
        "        family=\"binomial\"\n",
        "    )\n",
        "\n",
        "    print(\"ğŸ”„ Logistic Regression eÄŸitiliyor...\")\n",
        "    lr_model = lr.fit(train_df)\n",
        "    print(\"âœ… Logistic Regression eÄŸitimi tamamlandÄ±\")\n",
        "\n",
        "    return lr_model\n",
        "\n",
        "def train_naive_bayes(train_df):\n",
        "    \"\"\"Naive Bayes modeli eÄŸitir\"\"\"\n",
        "    nb = NaiveBayes(\n",
        "        featuresCol=\"features\",\n",
        "        labelCol=\"label\",\n",
        "        smoothing=1.0,\n",
        "        modelType=\"multinomial\"\n",
        "    )\n",
        "\n",
        "    print(\"ğŸ”„ Naive Bayes eÄŸitiliyor...\")\n",
        "    nb_model = nb.fit(train_df)\n",
        "    print(\"âœ… Naive Bayes eÄŸitimi tamamlandÄ±\")\n",
        "\n",
        "    return nb_model\n"
      ],
      "metadata": {
        "id": "EwlqXdx5ZMZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ========================\n",
        "# 6. MODEL DEÄERLENDÄ°RME\n",
        "# ========================\n",
        "def evaluate_model(model, test_df, model_name):\n",
        "    \"\"\"Model performansÄ±nÄ± deÄŸerlendirir\"\"\"\n",
        "\n",
        "    print(f\"\\nğŸ” {model_name} modeli deÄŸerlendiriliyor...\")\n",
        "\n",
        "    # Tahmin yap\n",
        "    predictions = model.transform(test_df)\n",
        "\n",
        "    # Binary evaluator (AUC)\n",
        "    binary_evaluator = BinaryClassificationEvaluator(\n",
        "        labelCol=\"label\",\n",
        "        rawPredictionCol=\"rawPrediction\",\n",
        "        metricName=\"areaUnderROC\"\n",
        "    )\n",
        "    auc = binary_evaluator.evaluate(predictions)\n",
        "\n",
        "    # Multiclass evaluator\n",
        "    multi_evaluator = MulticlassClassificationEvaluator(\n",
        "        labelCol=\"label\",\n",
        "        predictionCol=\"prediction\"\n",
        "    )\n",
        "\n",
        "    accuracy = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"accuracy\"})\n",
        "    precision = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"weightedPrecision\"})\n",
        "    recall = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"weightedRecall\"})\n",
        "    f1 = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"f1\"})\n",
        "\n",
        "    print(f\"\\nğŸ“Š {model_name} Model SonuÃ§larÄ±:\")\n",
        "    print(f\"   AUC-ROC: {auc:.4f}\")\n",
        "    print(f\"   Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"   Precision: {precision:.4f}\")\n",
        "    print(f\"   Recall: {recall:.4f}\")\n",
        "    print(f\"   F1-Score: {f1:.4f}\")\n",
        "\n",
        "    # Confusion Matrix\n",
        "    print(f\"\\nğŸ“Š {model_name} Confusion Matrix:\")\n",
        "    confusion_matrix = predictions.groupBy(\"label\", \"prediction\").count().orderBy(\"label\", \"prediction\")\n",
        "    confusion_matrix.show()\n",
        "\n",
        "    # DetaylÄ± sÄ±nÄ±f bazlÄ± metrikler\n",
        "    print(f\"\\nğŸ“Š {model_name} SÄ±nÄ±f BazlÄ± Metrikler:\")\n",
        "    for label_val in [0, 1]:\n",
        "        label_name = \"Yalan Haber\" if label_val == 0 else \"DoÄŸru Haber\"\n",
        "\n",
        "        tp = predictions.filter((col(\"label\") == label_val) & (col(\"prediction\") == label_val)).count()\n",
        "        fp = predictions.filter((col(\"label\") != label_val) & (col(\"prediction\") == label_val)).count()\n",
        "        fn = predictions.filter((col(\"label\") == label_val) & (col(\"prediction\") != label_val)).count()\n",
        "        tn = predictions.filter((col(\"label\") != label_val) & (col(\"prediction\") != label_val)).count()\n",
        "\n",
        "        precision_class = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        recall_class = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        f1_class = 2 * (precision_class * recall_class) / (precision_class + recall_class) if (precision_class + recall_class) > 0 else 0\n",
        "\n",
        "        print(f\"   {label_name}:\")\n",
        "        print(f\"      Precision: {precision_class:.4f}\")\n",
        "        print(f\"      Recall: {recall_class:.4f}\")\n",
        "        print(f\"      F1-Score: {f1_class:.4f}\")\n",
        "\n",
        "    return {\n",
        "        \"model_name\": model_name,\n",
        "        \"auc\": auc,\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1\n",
        "    }\n"
      ],
      "metadata": {
        "id": "GoS23IR_ZOBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# 7. MODEL KAYDETME VE YÃœKLEME\n",
        "# ========================\n",
        "def save_complete_pipeline(preprocessing_model, ml_model, model_path):\n",
        "    \"\"\"TÃ¼m pipeline'Ä± kaydeder\"\"\"\n",
        "\n",
        "    import os\n",
        "    os.makedirs(model_path, exist_ok=True)\n",
        "\n",
        "    # Preprocessing pipeline'Ä± kaydet\n",
        "    preprocessing_path = f\"{model_path}/preprocessing_pipeline\"\n",
        "    preprocessing_model.write().overwrite().save(preprocessing_path)\n",
        "\n",
        "    # ML modelini kaydet\n",
        "    ml_model_path = f\"{model_path}/ml_model\"\n",
        "    ml_model.write().overwrite().save(ml_model_path)\n",
        "\n",
        "    print(f\"âœ… TÃ¼m pipeline kaydedildi: {model_path}\")\n",
        "\n",
        "def load_complete_pipeline(spark, model_path):\n",
        "    \"\"\"KaydedilmiÅŸ pipeline'Ä± yÃ¼kler\"\"\"\n",
        "    from pyspark.ml import PipelineModel\n",
        "    from pyspark.ml.classification import LogisticRegressionModel, NaiveBayesModel\n",
        "\n",
        "    preprocessing_path = f\"{model_path}/preprocessing_pipeline\"\n",
        "    ml_model_path = f\"{model_path}/ml_model\"\n",
        "\n",
        "    # Pipeline'Ä± yÃ¼kle\n",
        "    preprocessing_pipeline = PipelineModel.load(preprocessing_path)\n",
        "\n",
        "    # ML modelini yÃ¼kle\n",
        "    try:\n",
        "        ml_model = LogisticRegressionModel.load(ml_model_path)\n",
        "        model_type = \"LogisticRegression\"\n",
        "    except:\n",
        "        try:\n",
        "            ml_model = NaiveBayesModel.load(ml_model_path)\n",
        "            model_type = \"NaiveBayes\"\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"Model yÃ¼klenemedi: {e}\")\n",
        "\n",
        "    print(f\"âœ… Pipeline yÃ¼klendi: {model_type}\")\n",
        "    return preprocessing_pipeline, ml_model, model_type\n"
      ],
      "metadata": {
        "id": "pZ3IqUNeZR33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# 8. YENÄ° METÄ°N TAHMÄ°NÄ°\n",
        "# ========================\n",
        "def predict_single_text(spark, preprocessing_pipeline, ml_model, text):\n",
        "    \"\"\"Tek bir metin iÃ§in tahmin yapar\"\"\"\n",
        "\n",
        "    # DataFrame oluÅŸtur\n",
        "    new_data = spark.createDataFrame([(text,)], [\"text\"])\n",
        "\n",
        "    # Ã–niÅŸleme uygula\n",
        "    processed_data = preprocessing_pipeline.transform(new_data)\n",
        "\n",
        "    # Tahmin yap\n",
        "    prediction = ml_model.transform(processed_data)\n",
        "\n",
        "    # Sonucu al\n",
        "    result = prediction.select(\"text\", \"prediction\", \"probability\").collect()[0]\n",
        "\n",
        "    pred_label = int(result.prediction)\n",
        "    probability = result.probability.toArray()\n",
        "    confidence = max(probability)\n",
        "\n",
        "    news_type = \"DOÄRU HABER\" if pred_label == 1 else \"YALAN HABER\"\n",
        "\n",
        "    print(f\"\\nğŸ” Tahmin Sonucu:\")\n",
        "    print(f\"   Metin: {text[:150]}...\")\n",
        "    print(f\"   SonuÃ§: {news_type}\")\n",
        "    print(f\"   GÃ¼ven: {confidence:.4f}\")\n",
        "    print(f\"   OlasÄ±lÄ±klar:\")\n",
        "    print(f\"      Yalan Haber: {probability[0]:.4f}\")\n",
        "    print(f\"      DoÄŸru Haber: {probability[1]:.4f}\")\n",
        "\n",
        "    return pred_label, confidence\n"
      ],
      "metadata": {
        "id": "M_wSJiB2ZUR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ========================\n",
        "# 9. BATCH TAHMÄ°N\n",
        "# ========================\n",
        "def predict_batch_texts(spark, preprocessing_pipeline, ml_model, texts):\n",
        "    \"\"\"Birden fazla metin iÃ§in toplu tahmin\"\"\"\n",
        "\n",
        "    # DataFrame oluÅŸtur\n",
        "    texts_df = spark.createDataFrame([(text,) for text in texts], [\"text\"])\n",
        "\n",
        "    # Ã–niÅŸleme uygula\n",
        "    processed_df = preprocessing_pipeline.transform(texts_df)\n",
        "\n",
        "    # Tahmin yap\n",
        "    predictions_df = ml_model.transform(processed_df)\n",
        "\n",
        "    # SonuÃ§larÄ± al\n",
        "    results = predictions_df.select(\"text\", \"prediction\", \"probability\").collect()\n",
        "\n",
        "    print(f\"\\nğŸ” Toplu Tahmin SonuÃ§larÄ± ({len(results)} metin):\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    batch_results = []\n",
        "    for i, result in enumerate(results, 1):\n",
        "        pred_label = int(result.prediction)\n",
        "        probability = result.probability.toArray()\n",
        "        confidence = max(probability)\n",
        "        news_type = \"DOÄRU\" if pred_label == 1 else \"YALAN\"\n",
        "\n",
        "        print(f\"{i}. Metin: {result.text[:100]}...\")\n",
        "        print(f\"   SonuÃ§: {news_type} HABER (GÃ¼ven: {confidence:.4f})\")\n",
        "\n",
        "        batch_results.append({\n",
        "            \"text\": result.text,\n",
        "            \"prediction\": pred_label,\n",
        "            \"confidence\": confidence,\n",
        "            \"probabilities\": probability\n",
        "        })\n",
        "\n",
        "    return batch_results\n"
      ],
      "metadata": {
        "id": "GVpKHEg0ZWj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ========================\n",
        "# 10. ANA FONKSÄ°YON\n",
        "# ========================\n",
        "def main():\n",
        "    \"\"\"Ana Ã§alÄ±ÅŸtÄ±rma fonksiyonu\"\"\"\n",
        "\n",
        "    # Spark baÅŸlat\n",
        "    spark = initialize_spark()\n",
        "\n",
        "    try:\n",
        "        # 1. CSV veri yÃ¼kleme\n",
        "        csv_path = \"etiketli_haberler.csv\"  # CSV dosyasÄ±nÄ±n yolu\n",
        "        df = load_csv_data(spark, csv_path)\n",
        "\n",
        "        # 2. Veri keÅŸfi\n",
        "        explore_data(df)\n",
        "\n",
        "        # 3. Train-Test split\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"ğŸ“Š VERÄ° BÃ–LME\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "        train_count = train_df.count()\n",
        "        test_count = test_df.count()\n",
        "\n",
        "        print(f\"ğŸ“Š EÄŸitim seti: {train_count}\")\n",
        "        print(f\"ğŸ“Š Test seti: {test_count}\")\n",
        "\n",
        "        # EÄŸitim setinde etiket daÄŸÄ±lÄ±mÄ±\n",
        "        print(\"\\nğŸ“Š EÄŸitim Seti Etiket DaÄŸÄ±lÄ±mÄ±:\")\n",
        "        train_df.groupBy(\"label\").count().orderBy(\"label\").show()\n",
        "\n",
        "        # 4. Ã–niÅŸleme pipeline'Ä±\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"ğŸ”§ Ã–NÄ°ÅLEME PÄ°PELÄ°NE\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        preprocessing_pipeline = create_preprocessing_pipeline()\n",
        "        preprocessing_model = preprocessing_pipeline.fit(train_df)\n",
        "\n",
        "        # Ã–niÅŸlemeyi uygula\n",
        "        train_processed = preprocessing_model.transform(train_df).cache()\n",
        "        test_processed = preprocessing_model.transform(test_df).cache()\n",
        "\n",
        "        print(\"âœ… Ã–niÅŸleme tamamlandÄ±\")\n",
        "\n",
        "        # 5. Model eÄŸitimi ve karÅŸÄ±laÅŸtÄ±rma\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"ğŸ¯ MODEL EÄÄ°TÄ°MÄ° VE KARÅILAÅTIRMA\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        # Logistic Regression\n",
        "        lr_model = train_logistic_regression(train_processed)\n",
        "        lr_results = evaluate_model(lr_model, test_processed, \"Logistic Regression\")\n",
        "\n",
        "        # Naive Bayes\n",
        "        nb_model = train_naive_bayes(train_processed)\n",
        "        nb_results = evaluate_model(nb_model, test_processed, \"Naive Bayes\")\n",
        "\n",
        "        # 6. En iyi modeli seÃ§\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"ğŸ† MODEL KARÅILAÅTIRMA\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        models_comparison = [lr_results, nb_results]\n",
        "        best_result = max(models_comparison, key=lambda x: x[\"f1\"])\n",
        "        best_model = lr_model if best_result[\"model_name\"] == \"Logistic Regression\" else nb_model\n",
        "\n",
        "        print(f\"ğŸ† En iyi model: {best_result['model_name']}\")\n",
        "        print(f\"   F1-Score: {best_result['f1']:.4f}\")\n",
        "        print(f\"   Accuracy: {best_result['accuracy']:.4f}\")\n",
        "        print(f\"   AUC-ROC: {best_result['auc']:.4f}\")\n",
        "\n",
        "        # 7. Model kaydetme\n",
        "        import traceback\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"ğŸ’¾ MODEL KAYDETME\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        model_save_path = \"./fake_news_model\"\n",
        "\n",
        "        try:\n",
        "            save_complete_pipeline(preprocessing_model, best_model, model_save_path)\n",
        "        except Exception as e:\n",
        "            print(\"âŒ Hata oluÅŸtu:\")\n",
        "            traceback.print_exc()  # Hata detaylarÄ±nÄ± tam gÃ¶sterir\n",
        "\n",
        "        # 8. KaydedilmiÅŸ modeli test et\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"ğŸ”„ KAYDEDÄ°LMÄ°Å MODEL TESTÄ°\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        loaded_preprocessing, loaded_model, model_type = load_complete_pipeline(spark, model_save_path)\n",
        "\n",
        "        # 9. Ã–rnek tahminler\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"ğŸ§ª Ã–RNEK TAHMÄ°NLER\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        sample_texts = [\n",
        "            \"CumhurbaÅŸkanÄ± bugÃ¼n yeni ekonomik reformlarÄ± aÃ§Ä±kladÄ± ve enflasyonla mÃ¼cadele planÄ±nÄ± duyurdu.\",\n",
        "            \"Åok eden iddia: 5G kulelerinin koronavirÃ¼s yaydÄ±ÄŸÄ± bilimsel olarak kanÄ±tlandÄ±!\",\n",
        "            \"TÃ¼rkiye'nin ihracat rakamlarÄ± geÃ§en yÄ±la gÃ¶re yÃ¼zde 15 artÄ±ÅŸ gÃ¶sterdi.\",\n",
        "            \"Ä°nanÄ±lmaz keÅŸif: Bu bitki suyu iÃ§erek 1 ayda 20 kilo verebilirsiniz!\",\n",
        "            \"Merkez BankasÄ± faiz oranlarÄ±nÄ± deÄŸiÅŸtirmeme kararÄ± aldÄ±.\",\n",
        "            \"Uzmanlar uyarÄ±yor: Cep telefonu kullanÄ±mÄ± beyin kanserine neden oluyor!\"\n",
        "        ]\n",
        "\n",
        "        # Tekli tahminler\n",
        "        for i, text in enumerate(sample_texts, 1):\n",
        "            print(f\"\\n--- Ã–rnek {i} ---\")\n",
        "            predict_single_text(spark, loaded_preprocessing, loaded_model, text)\n",
        "\n",
        "        # Toplu tahmin\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"ğŸ“Š TOPLU TAHMÄ°N TESTÄ°\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        batch_results = predict_batch_texts(spark, loaded_preprocessing, loaded_model, sample_texts[:3])\n",
        "\n",
        "        print(\"\\nâœ… TÃ¼m iÅŸlemler baÅŸarÄ±yla tamamlandÄ±!\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Hata: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "\n",
        "    finally:\n",
        "        # Spark'Ä± kapat\n",
        "        spark.stop()\n",
        "        print(\"\\nğŸ”„ Spark session sonlandÄ±rÄ±ldÄ±\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQXhNfjTZY1F",
        "outputId": "8db93c84-0077-4b53-9176-adeb731f7ce3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Spark Session baÅŸlatÄ±ldÄ±\n",
            "ğŸ“‹ Spark Version: 3.5.1\n",
            "ğŸ“– CSV dosyasÄ± okunuyor: etiketli_haberler.csv\n",
            "ğŸ“‹ Bulunan kolonlar: ['metin', 'etiket']\n",
            "ğŸ“ Metin kolonu: metin\n",
            "ğŸ·ï¸ Etiket kolonu: etiket\n",
            "ğŸ“Š Unique etiketler: [1, 0]\n",
            "ğŸ” Ã–rnek etiket deÄŸerleri: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "ğŸ“Š Etiketler zaten sayÄ±sal\n",
            "âœ… Toplam 4458 haber yÃ¼klendi\n",
            "   - DoÄŸru Haber: <built-in method count of Row object at 0x7af8680efdd0>\n",
            "   - Yalan Haber: <built-in method count of Row object at 0x7af8680ec180>\n",
            "\n",
            "==================================================\n",
            "ğŸ“Š VERÄ° KEÅFÄ° VE ANALÄ°ZÄ°\n",
            "==================================================\n",
            "ğŸ“‹ Temel Ä°statistikler:\n",
            "+-------+--------------------+-------------------+\n",
            "|summary|                text|              label|\n",
            "+-------+--------------------+-------------------+\n",
            "|  count|                4458|               4458|\n",
            "|   mean|                NULL| 0.5148048452220727|\n",
            "| stddev|                NULL|0.49983683229829845|\n",
            "|    min|a haber kar yaÄŸÄ±ÅŸ...|                  0|\n",
            "|    max|ÅŸÄ±rnaktan acÄ± hab...|                  1|\n",
            "+-------+--------------------+-------------------+\n",
            "\n",
            "ğŸ“Š Etiket DaÄŸÄ±lÄ±mÄ±:\n",
            "+-----+-----+\n",
            "|label|count|\n",
            "+-----+-----+\n",
            "|    0| 2163|\n",
            "|    1| 2295|\n",
            "+-----+-----+\n",
            "\n",
            "ğŸ“ Metin Uzunluk Ä°statistikleri:\n",
            "+-------+------------------+\n",
            "|summary|       text_length|\n",
            "+-------+------------------+\n",
            "|  count|              4458|\n",
            "|   mean| 2114.125616868551|\n",
            "| stddev|1571.3156224018376|\n",
            "|    min|                17|\n",
            "|    max|             32584|\n",
            "+-------+------------------+\n",
            "\n",
            "ğŸ“ Ã–rnek Metinler:\n",
            "\n",
            "1. DoÄŸru Haber:\n",
            "   tÃ¼rkiyenin en bÃ¼yÃ¼ÄŸÃ¼ ve kapaklar aÃ§Ä±ldÄ± gÃ¼neydoÄŸu anadolu projesi gap kapsamÄ±nda inÅŸa edilen dÃ¼nyanÄ±n beÅŸinci tÃ¼rkiyenin de en uzun sulama tÃ¼neli olan suruÃ§ tÃ¼neli ile yaklaÅŸÄ±k yÄ±l Ã¶nce sulanmaya baÅŸl...\n",
            "\n",
            "2. DoÄŸru Haber:\n",
            "   krem ÅŸanti nasÄ±l yapÄ±lÄ±r doÄŸal ve katkÄ±sÄ±z krem ÅŸanti tarifleri krem ÅŸanti pastalarÄ±n ve tatlÄ±larÄ±n olmazsa olmaz malzemelerinden marketten alÄ±nan toz krem ÅŸantiler Ã§ok kÄ±sa sÃ¼re iÃ§inde tÃ¼ketime hazÄ±r...\n",
            "\n",
            "3. DoÄŸru Haber:\n",
            "   yanlÄ±ÅŸ anlaÅŸÄ±lan mesaj vatandaÅŸlarÄ± sokaÄŸa dÃ¶ktÃ¼ kÃ¼tahya da doÄŸalgaz daÄŸÄ±tÄ±mÄ± yapan yetkili firma tarafÄ±ndan gÃ¶nderilen borcunuz var mesajÄ± Ã¼zerine Ã§ok sayÄ±da vatandaÅŸ firma binasÄ± Ã¶nÃ¼nde toplandÄ±...\n",
            "ğŸ” Null DeÄŸer KontrolÃ¼:\n",
            "   Text null: 0\n",
            "   Label null: 0\n",
            "\n",
            "==================================================\n",
            "ğŸ“Š VERÄ° BÃ–LME\n",
            "==================================================\n",
            "ğŸ“Š EÄŸitim seti: 3610\n",
            "ğŸ“Š Test seti: 848\n",
            "\n",
            "ğŸ“Š EÄŸitim Seti Etiket DaÄŸÄ±lÄ±mÄ±:\n",
            "+-----+-----+\n",
            "|label|count|\n",
            "+-----+-----+\n",
            "|    0| 1734|\n",
            "|    1| 1876|\n",
            "+-----+-----+\n",
            "\n",
            "\n",
            "==================================================\n",
            "ğŸ”§ Ã–NÄ°ÅLEME PÄ°PELÄ°NE\n",
            "==================================================\n",
            "ğŸ”§ Metin Ã¶niÅŸleme pipeline'Ä± oluÅŸturuluyor...\n",
            "âœ… Metin Ã¶niÅŸleme pipeline'Ä± oluÅŸturuldu\n",
            "âœ… Ã–niÅŸleme tamamlandÄ±\n",
            "\n",
            "==================================================\n",
            "ğŸ¯ MODEL EÄÄ°TÄ°MÄ° VE KARÅILAÅTIRMA\n",
            "==================================================\n",
            "ğŸ”„ Logistic Regression eÄŸitiliyor...\n",
            "âœ… Logistic Regression eÄŸitimi tamamlandÄ±\n",
            "\n",
            "ğŸ” Logistic Regression modeli deÄŸerlendiriliyor...\n",
            "\n",
            "ğŸ“Š Logistic Regression Model SonuÃ§larÄ±:\n",
            "   AUC-ROC: 0.9565\n",
            "   Accuracy: 0.8667\n",
            "   Precision: 0.8839\n",
            "   Recall: 0.8667\n",
            "   F1-Score: 0.8654\n",
            "\n",
            "ğŸ“Š Logistic Regression Confusion Matrix:\n",
            "+-----+----------+-----+\n",
            "|label|prediction|count|\n",
            "+-----+----------+-----+\n",
            "|    0|       0.0|  328|\n",
            "|    0|       1.0|  101|\n",
            "|    1|       0.0|   12|\n",
            "|    1|       1.0|  407|\n",
            "+-----+----------+-----+\n",
            "\n",
            "\n",
            "ğŸ“Š Logistic Regression SÄ±nÄ±f BazlÄ± Metrikler:\n",
            "   Yalan Haber:\n",
            "      Precision: 0.9647\n",
            "      Recall: 0.7646\n",
            "      F1-Score: 0.8531\n",
            "   DoÄŸru Haber:\n",
            "      Precision: 0.8012\n",
            "      Recall: 0.9714\n",
            "      F1-Score: 0.8781\n",
            "ğŸ”„ Naive Bayes eÄŸitiliyor...\n",
            "âœ… Naive Bayes eÄŸitimi tamamlandÄ±\n",
            "\n",
            "ğŸ” Naive Bayes modeli deÄŸerlendiriliyor...\n",
            "\n",
            "ğŸ“Š Naive Bayes Model SonuÃ§larÄ±:\n",
            "   AUC-ROC: 0.8058\n",
            "   Accuracy: 0.9222\n",
            "   Precision: 0.9296\n",
            "   Recall: 0.9222\n",
            "   F1-Score: 0.9218\n",
            "\n",
            "ğŸ“Š Naive Bayes Confusion Matrix:\n",
            "+-----+----------+-----+\n",
            "|label|prediction|count|\n",
            "+-----+----------+-----+\n",
            "|    0|       0.0|  424|\n",
            "|    0|       1.0|    5|\n",
            "|    1|       0.0|   61|\n",
            "|    1|       1.0|  358|\n",
            "+-----+----------+-----+\n",
            "\n",
            "\n",
            "ğŸ“Š Naive Bayes SÄ±nÄ±f BazlÄ± Metrikler:\n",
            "   Yalan Haber:\n",
            "      Precision: 0.8742\n",
            "      Recall: 0.9883\n",
            "      F1-Score: 0.9278\n",
            "   DoÄŸru Haber:\n",
            "      Precision: 0.9862\n",
            "      Recall: 0.8544\n",
            "      F1-Score: 0.9156\n",
            "\n",
            "==================================================\n",
            "ğŸ† MODEL KARÅILAÅTIRMA\n",
            "==================================================\n",
            "ğŸ† En iyi model: Naive Bayes\n",
            "   F1-Score: 0.9218\n",
            "   Accuracy: 0.9222\n",
            "   AUC-ROC: 0.8058\n",
            "\n",
            "==================================================\n",
            "ğŸ’¾ MODEL KAYDETME\n",
            "==================================================\n",
            "âœ… TÃ¼m pipeline kaydedildi: ./fake_news_model\n",
            "\n",
            "==================================================\n",
            "ğŸ”„ KAYDEDÄ°LMÄ°Å MODEL TESTÄ°\n",
            "==================================================\n",
            "âœ… Pipeline yÃ¼klendi: NaiveBayes\n",
            "\n",
            "==================================================\n",
            "ğŸ§ª Ã–RNEK TAHMÄ°NLER\n",
            "==================================================\n",
            "\n",
            "--- Ã–rnek 1 ---\n",
            "ğŸ“¢ Tahmin: GerÃ§ek Haber âœ…\n",
            "\n",
            "--- Ã–rnek 2 ---\n",
            "ğŸ“¢ Tahmin: Yalan Haber âŒ\n",
            "\n",
            "--- Ã–rnek 3 ---\n",
            "ğŸ“¢ Tahmin: GerÃ§ek Haber âœ…\n",
            "\n",
            "--- Ã–rnek 4 ---\n",
            "ğŸ“¢ Tahmin: Yalan Haber âŒ\n",
            "\n",
            "--- Ã–rnek 5 ---\n",
            "ğŸ“¢ Tahmin: GerÃ§ek Haber âœ…\n",
            "\n",
            "--- Ã–rnek 6 ---\n",
            "ğŸ“¢ Tahmin: Yalan Haber âŒ\n",
            "\n",
            "==================================================\n",
            "ğŸ“Š TOPLU TAHMÄ°N TESTÄ°\n",
            "==================================================\n",
            "\n",
            "ğŸ” Toplu Tahmin SonuÃ§larÄ± (3 metin):\n",
            "--------------------------------------------------------------------------------\n",
            "1. Metin: CumhurbaÅŸkanÄ± bugÃ¼n yeni ekonomik reformlarÄ± aÃ§Ä±kladÄ± ve enflasyonla mÃ¼cadele planÄ±nÄ± duyurdu....\n",
            "   SonuÃ§: DOÄRU HABER (GÃ¼ven: 0.9453)\n",
            "2. Metin: Åok eden iddia: 5G kulelerinin koronavirÃ¼s yaydÄ±ÄŸÄ± bilimsel olarak kanÄ±tlandÄ±!...\n",
            "   SonuÃ§: YALAN HABER (GÃ¼ven: 1.0000)\n",
            "3. Metin: TÃ¼rkiye'nin ihracat rakamlarÄ± geÃ§en yÄ±la gÃ¶re yÃ¼zde 15 artÄ±ÅŸ gÃ¶sterdi....\n",
            "   SonuÃ§: DOÄRU HABER (GÃ¼ven: 0.9987)\n",
            "\n",
            "âœ… TÃ¼m iÅŸlemler baÅŸarÄ±yla tamamlandÄ±!\n",
            "\n",
            "ğŸ”„ Spark session sonlandÄ±rÄ±ldÄ±\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r fake_news_model.zip fake_news_model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SNqxFqedhrR",
        "outputId": "041ade69-3e91-4c78-9f79-d0761feaaea0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: fake_news_model/ (stored 0%)\n",
            "  adding: fake_news_model/ml_model/ (stored 0%)\n",
            "  adding: fake_news_model/ml_model/metadata/ (stored 0%)\n",
            "  adding: fake_news_model/ml_model/metadata/part-00000 (deflated 48%)\n",
            "  adding: fake_news_model/ml_model/metadata/_SUCCESS (stored 0%)\n",
            "  adding: fake_news_model/ml_model/metadata/.part-00000.crc (stored 0%)\n",
            "  adding: fake_news_model/ml_model/metadata/._SUCCESS.crc (stored 0%)\n",
            "  adding: fake_news_model/ml_model/data/ (stored 0%)\n",
            "  adding: fake_news_model/ml_model/data/part-00000-1fdc2f49-5e3e-4a87-aec2-a105ca894ca5-c000.snappy.parquet (deflated 9%)\n",
            "  adding: fake_news_model/ml_model/data/.part-00000-1fdc2f49-5e3e-4a87-aec2-a105ca894ca5-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: fake_news_model/ml_model/data/_SUCCESS (stored 0%)\n",
            "  adding: fake_news_model/ml_model/data/._SUCCESS.crc (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/ (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/ (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/2_HashingTF_4fb752621ebf/ (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/2_HashingTF_4fb752621ebf/metadata/ (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/2_HashingTF_4fb752621ebf/metadata/part-00000 (deflated 37%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/2_HashingTF_4fb752621ebf/metadata/_SUCCESS (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/2_HashingTF_4fb752621ebf/metadata/.part-00000.crc (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/2_HashingTF_4fb752621ebf/metadata/._SUCCESS.crc (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/1_StopWordsRemover_9e616728d4ca/ (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/1_StopWordsRemover_9e616728d4ca/metadata/ (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/1_StopWordsRemover_9e616728d4ca/metadata/part-00000 (deflated 61%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/1_StopWordsRemover_9e616728d4ca/metadata/_SUCCESS (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/1_StopWordsRemover_9e616728d4ca/metadata/.part-00000.crc (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/1_StopWordsRemover_9e616728d4ca/metadata/._SUCCESS.crc (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/3_IDF_eefdce583c81/ (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/3_IDF_eefdce583c81/metadata/ (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/3_IDF_eefdce583c81/metadata/part-00000 (deflated 35%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/3_IDF_eefdce583c81/metadata/_SUCCESS (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/3_IDF_eefdce583c81/metadata/.part-00000.crc (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/3_IDF_eefdce583c81/metadata/._SUCCESS.crc (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/3_IDF_eefdce583c81/data/ (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/3_IDF_eefdce583c81/data/part-00000-e21e12bc-0711-4259-8d0c-1a0717c11ba3-c000.snappy.parquet (deflated 50%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/3_IDF_eefdce583c81/data/.part-00000-e21e12bc-0711-4259-8d0c-1a0717c11ba3-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/3_IDF_eefdce583c81/data/_SUCCESS (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/3_IDF_eefdce583c81/data/._SUCCESS.crc (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/0_Tokenizer_ea73d0c7c743/ (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/0_Tokenizer_ea73d0c7c743/metadata/ (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/0_Tokenizer_ea73d0c7c743/metadata/part-00000 (deflated 34%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/0_Tokenizer_ea73d0c7c743/metadata/_SUCCESS (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/0_Tokenizer_ea73d0c7c743/metadata/.part-00000.crc (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/0_Tokenizer_ea73d0c7c743/metadata/._SUCCESS.crc (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/metadata/ (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/metadata/part-00000 (deflated 22%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/metadata/_SUCCESS (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/metadata/.part-00000.crc (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/metadata/._SUCCESS.crc (stored 0%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"fake_news_model.zip\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "QXBd3DYXdpGS",
        "outputId": "545471c3-0e6a-431e-c39f-054f1a7301d1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_1d71e369-1f55-4e15-b100-ccb332c87ef7\", \"fake_news_model.zip\", 166245)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}