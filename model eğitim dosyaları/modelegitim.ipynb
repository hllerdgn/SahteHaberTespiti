{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0teI28HTjS3"
      },
      "outputs": [],
      "source": [
        "# PySpark ile CSV'den Yalan Haber Tespiti - Tam Pipeline\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer\n",
        "from pyspark.ml.classification import LogisticRegression, NaiveBayes\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.sql.types import DoubleType\n",
        "from builtins import max\n",
        "\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. SPARK SESSION BAŞLATMA\n",
        "# ========================\n",
        "def initialize_spark():\n",
        "    \"\"\"Spark session başlatır - CSV için optimize edilmiş\"\"\"\n",
        "\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"FakeNewsDetection_CSV\") \\\n",
        "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\") \\\n",
        "        .config(\"spark.driver.memory\", \"4g\") \\\n",
        "        .config(\"spark.executor.memory\", \"4g\") \\\n",
        "        .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
        "        .config(\"spark.network.timeout\", \"300s\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "    spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "    print(\"✅ Spark Session başlatıldı\")\n",
        "    print(f\"📋 Spark Version: {spark.version}\")\n",
        "\n",
        "    return spark\n"
      ],
      "metadata": {
        "id": "oGWNzR_qT4iO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ========================\n",
        "# 2. CSV VERİ YÜKLEME\n",
        "# ========================\n",
        "def load_csv_data(spark, csv_path):\n",
        "    \"\"\"\n",
        "    CSV dosyasından veri yükler\n",
        "\n",
        "    Args:\n",
        "        spark: SparkSession\n",
        "        csv_path: CSV dosyasının yolu\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: text ve label kolonları içeren DataFrame\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"📖 CSV dosyası okunuyor: {csv_path}\")\n",
        "\n",
        "    try:\n",
        "        # CSV'yi Spark DataFrame olarak oku\n",
        "        df = spark.read \\\n",
        "            .option(\"header\", \"true\") \\\n",
        "            .option(\"inferSchema\", \"true\") \\\n",
        "            .option(\"encoding\", \"UTF-8\") \\\n",
        "            .option(\"multiline\", \"true\") \\\n",
        "            .option(\"escape\", '\"') \\\n",
        "            .csv(csv_path)\n",
        "\n",
        "        # Kolon isimlerini kontrol et ve standartlaştır\n",
        "        columns = df.columns\n",
        "        print(f\"📋 Bulunan kolonlar: {columns}\")\n",
        "\n",
        "        # Kolon isimlerini temizle (boşluk, özel karakter kaldır)\n",
        "        for col_name in columns:\n",
        "            clean_name = col_name.strip().lower()\n",
        "            if col_name != clean_name:\n",
        "                df = df.withColumnRenamed(col_name, clean_name)\n",
        "\n",
        "        # Kolon isimlerini standart hale getir\n",
        "        columns = df.columns\n",
        "        text_col = None\n",
        "        label_col = None\n",
        "\n",
        "        # Metin kolonu bulma\n",
        "        for column in columns:\n",
        "            if any(keyword in column.lower() for keyword in ['metin', 'text', ...]):\n",
        "                text_col = column\n",
        "                break\n",
        "\n",
        "\n",
        "        # Etiket kolonu bulma\n",
        "        for column in columns:\n",
        "            if any(keyword in column.lower() for keyword in ['etiket', 'label']):\n",
        "                label_col = column\n",
        "                break\n",
        "\n",
        "        if not text_col or not label_col:\n",
        "            print(f\"⚠️ Uygun kolonlar bulunamadı. Mevcut kolonlar: {columns}\")\n",
        "            print(\"💡 İlk kolonu metin, ikinci kolonu etiket olarak kullanacağım.\")\n",
        "            text_col = columns[0]\n",
        "            label_col = columns[1]\n",
        "\n",
        "        print(f\"📝 Metin kolonu: {text_col}\")\n",
        "        print(f\"🏷️ Etiket kolonu: {label_col}\")\n",
        "\n",
        "        # Kolonları yeniden adlandır\n",
        "        df = df.select(\n",
        "            col(text_col).alias(\"text\"),\n",
        "            col(label_col).alias(\"label_raw\")\n",
        "        )\n",
        "\n",
        "        # Null değerleri filtrele\n",
        "        df = df.filter(col(\"text\").isNotNull() & col(\"label_raw\").isNotNull())\n",
        "        df = df.filter((col(\"text\") != \"\") & (length(col(\"text\")) > 10))\n",
        "\n",
        "        # Etiketleri kontrol et ve dönüştür\n",
        "        unique_labels = df.select(\"label_raw\").distinct().collect()\n",
        "        print(f\"📊 Unique etiketler: {[row.label_raw for row in unique_labels]}\")\n",
        "\n",
        "        # String etiketleri sayısal değerlere çevir\n",
        "        df = convert_labels_to_numeric(df)\n",
        "\n",
        "        # Veri istatistikleri\n",
        "        total_count = df.count()\n",
        "        label_counts = df.groupBy(\"label\").count().collect()\n",
        "\n",
        "        print(f\"✅ Toplam {total_count} haber yüklendi\")\n",
        "        for row in label_counts:\n",
        "            label_name = \"Doğru Haber\" if row.label == 1 else \"Yalan Haber\"\n",
        "            print(f\"   - {label_name}: {row.count}\")\n",
        "\n",
        "        # DataFrame'i optimize et\n",
        "        df = df.coalesce(4).cache()\n",
        "\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ CSV okuma hatası: {e}\")\n",
        "        raise\n",
        "\n",
        "def convert_labels_to_numeric(df):\n",
        "    \"\"\"Etiketleri sayısal değerlere çevirir\"\"\"\n",
        "\n",
        "    # Önce etiket türünü kontrol et\n",
        "    sample_labels = df.select(\"label_raw\").limit(10).collect()\n",
        "    sample_values = [row.label_raw for row in sample_labels]\n",
        "\n",
        "    print(f\"🔍 Örnek etiket değerleri: {sample_values}\")\n",
        "\n",
        "    # Eğer zaten sayısal ise\n",
        "    if all(isinstance(val, (int, float)) for val in sample_values if val is not None):\n",
        "        print(\"📊 Etiketler zaten sayısal\")\n",
        "        # 0 ve 1'e normalize et\n",
        "        df = df.withColumn(\"label\",\n",
        "                          when(col(\"label_raw\") > 0, 1).otherwise(0))\n",
        "    else:\n",
        "        # String etiketleri dönüştür\n",
        "        print(\"🔄 String etiketler sayısal değerlere çevriliyor\")\n",
        "\n",
        "        # Yaygın etiket türleri için mapping\n",
        "        df = df.withColumn(\"label\",\n",
        "            when(lower(col(\"label_raw\")).isin([\"true\", \"doğru\", \"gerçek\", \"real\", \"1\", \"dogru\"]), 1)\n",
        "            .when(lower(col(\"label_raw\")).isin([\"false\", \"yalan\", \"sahte\", \"fake\", \"0\"]), 0)\n",
        "            .otherwise(\n",
        "                when(col(\"label_raw\").cast(\"int\").isNotNull(),\n",
        "                     when(col(\"label_raw\").cast(\"int\") > 0, 1).otherwise(0))\n",
        "                .otherwise(0)  # Bilinmeyen değerleri 0 yap\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return df.drop(\"label_raw\")\n"
      ],
      "metadata": {
        "id": "BsVQgUiRUEZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# 3. VERİ KEŞFİ VE ANALİZİ\n",
        "# ========================\n",
        "def explore_data(df):\n",
        "    \"\"\"Veri keşfi ve analizi\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"📊 VERİ KEŞFİ VE ANALİZİ\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Temel istatistikler\n",
        "    print(\"📋 Temel İstatistikler:\")\n",
        "    df.describe().show()\n",
        "\n",
        "    # Etiket dağılımı\n",
        "    print(\"📊 Etiket Dağılımı:\")\n",
        "    df.groupBy(\"label\").count().orderBy(\"label\").show()\n",
        "\n",
        "    # Metin uzunluk istatistikleri\n",
        "    print(\"📏 Metin Uzunluk İstatistikleri:\")\n",
        "    df.select(\n",
        "        length(col(\"text\")).alias(\"text_length\")\n",
        "    ).describe().show()\n",
        "\n",
        "    # Örnek metinler\n",
        "    print(\"📝 Örnek Metinler:\")\n",
        "    sample_texts = df.select(\"text\", \"label\").limit(3).collect()\n",
        "    for i, row in enumerate(sample_texts, 1):\n",
        "        label_name = \"Doğru\" if row.label == 1 else \"Yalan\"\n",
        "        print(f\"\\n{i}. {label_name} Haber:\")\n",
        "        print(f\"   {row.text[:200]}...\")\n",
        "\n",
        "    # Null değer kontrolü\n",
        "    print(\"🔍 Null Değer Kontrolü:\")\n",
        "    null_counts = df.select(\n",
        "        sum(col(\"text\").isNull().cast(\"int\")).alias(\"text_nulls\"),\n",
        "        sum(col(\"label\").isNull().cast(\"int\")).alias(\"label_nulls\")\n",
        "    ).collect()[0]\n",
        "\n",
        "    print(f\"   Text null: {null_counts.text_nulls}\")\n",
        "    print(f\"   Label null: {null_counts.label_nulls}\")\n"
      ],
      "metadata": {
        "id": "YvQSWgpaZFjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# 4. METİN ÖN İŞLEME PİPELİNE\n",
        "# ========================\n",
        "def create_preprocessing_pipeline():\n",
        "    \"\"\"Metin önişleme pipeline'ı oluşturur\"\"\"\n",
        "\n",
        "    print(\"🔧 Metin önişleme pipeline'ı oluşturuluyor...\")\n",
        "\n",
        "    # 1. Tokenizasyon\n",
        "    tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "\n",
        "    # 2. Stop words kaldırma (Türkçe + İngilizce)\n",
        "    turkish_stopwords = [\n",
        "        \"ve\", \"ile\", \"bu\", \"şu\", \"o\", \"da\", \"de\", \"ta\", \"te\", \"ki\", \"mi\", \"mu\", \"mü\",\n",
        "        \"için\", \"gibi\", \"kadar\", \"sonra\", \"önce\", \"üzere\", \"göre\", \"karşı\", \"rağmen\",\n",
        "        \"bir\", \"iki\", \"üç\", \"dört\", \"beş\", \"altı\", \"yedi\", \"sekiz\", \"dokuz\", \"on\",\n",
        "        \"ben\", \"sen\", \"biz\", \"siz\", \"onlar\", \"benim\", \"senin\", \"bizim\", \"sizin\",\n",
        "        \"bunun\", \"şunun\", \"onun\", \"bunlar\", \"şunlar\", \"olan\", \"oldu\", \"olur\", \"olsa\",\n",
        "        \"daha\", \"çok\", \"az\", \"en\", \"hem\", \"ya\", \"veya\", \"ama\", \"fakat\", \"ancak\",\n",
        "        \"her\", \"hiç\", \"kendi\", \"tüm\", \"bütün\", \"hangi\", \"nasıl\", \"neden\", \"niçin\"\n",
        "    ]\n",
        "\n",
        "    english_stopwords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
        "    all_stopwords = list(set(turkish_stopwords + english_stopwords))\n",
        "\n",
        "    stop_remover = StopWordsRemover(\n",
        "        inputCol=\"words\",\n",
        "        outputCol=\"filtered_words\",\n",
        "        stopWords=all_stopwords\n",
        "    )\n",
        "\n",
        "    # 3. TF-IDF\n",
        "    hashing_tf = HashingTF(\n",
        "        inputCol=\"filtered_words\",\n",
        "        outputCol=\"raw_features\",\n",
        "        numFeatures=20000  # Özellik sayısını artırdık\n",
        "    )\n",
        "\n",
        "    idf = IDF(\n",
        "        inputCol=\"raw_features\",\n",
        "        outputCol=\"features\",\n",
        "        minDocFreq=2  # En az 2 dokümanda geçen terimleri al\n",
        "    )\n",
        "\n",
        "    # Pipeline oluştur\n",
        "    preprocessing_pipeline = Pipeline(stages=[\n",
        "        tokenizer,\n",
        "        stop_remover,\n",
        "        hashing_tf,\n",
        "        idf\n",
        "    ])\n",
        "\n",
        "    print(\"✅ Metin önişleme pipeline'ı oluşturuldu\")\n",
        "    return preprocessing_pipeline\n"
      ],
      "metadata": {
        "id": "2j67fI3uZIlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ========================\n",
        "# 5. MODEL EĞİTİMİ\n",
        "# ========================\n",
        "def train_logistic_regression(train_df):\n",
        "    \"\"\"Logistic Regression modeli eğitir\"\"\"\n",
        "    lr = LogisticRegression(\n",
        "        featuresCol=\"features\",\n",
        "        labelCol=\"label\",\n",
        "        maxIter=100,\n",
        "        regParam=0.1,\n",
        "        elasticNetParam=0.8,\n",
        "        family=\"binomial\"\n",
        "    )\n",
        "\n",
        "    print(\"🔄 Logistic Regression eğitiliyor...\")\n",
        "    lr_model = lr.fit(train_df)\n",
        "    print(\"✅ Logistic Regression eğitimi tamamlandı\")\n",
        "\n",
        "    return lr_model\n",
        "\n",
        "def train_naive_bayes(train_df):\n",
        "    \"\"\"Naive Bayes modeli eğitir\"\"\"\n",
        "    nb = NaiveBayes(\n",
        "        featuresCol=\"features\",\n",
        "        labelCol=\"label\",\n",
        "        smoothing=1.0,\n",
        "        modelType=\"multinomial\"\n",
        "    )\n",
        "\n",
        "    print(\"🔄 Naive Bayes eğitiliyor...\")\n",
        "    nb_model = nb.fit(train_df)\n",
        "    print(\"✅ Naive Bayes eğitimi tamamlandı\")\n",
        "\n",
        "    return nb_model\n"
      ],
      "metadata": {
        "id": "EwlqXdx5ZMZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ========================\n",
        "# 6. MODEL DEĞERLENDİRME\n",
        "# ========================\n",
        "def evaluate_model(model, test_df, model_name):\n",
        "    \"\"\"Model performansını değerlendirir\"\"\"\n",
        "\n",
        "    print(f\"\\n🔍 {model_name} modeli değerlendiriliyor...\")\n",
        "\n",
        "    # Tahmin yap\n",
        "    predictions = model.transform(test_df)\n",
        "\n",
        "    # Binary evaluator (AUC)\n",
        "    binary_evaluator = BinaryClassificationEvaluator(\n",
        "        labelCol=\"label\",\n",
        "        rawPredictionCol=\"rawPrediction\",\n",
        "        metricName=\"areaUnderROC\"\n",
        "    )\n",
        "    auc = binary_evaluator.evaluate(predictions)\n",
        "\n",
        "    # Multiclass evaluator\n",
        "    multi_evaluator = MulticlassClassificationEvaluator(\n",
        "        labelCol=\"label\",\n",
        "        predictionCol=\"prediction\"\n",
        "    )\n",
        "\n",
        "    accuracy = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"accuracy\"})\n",
        "    precision = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"weightedPrecision\"})\n",
        "    recall = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"weightedRecall\"})\n",
        "    f1 = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"f1\"})\n",
        "\n",
        "    print(f\"\\n📊 {model_name} Model Sonuçları:\")\n",
        "    print(f\"   AUC-ROC: {auc:.4f}\")\n",
        "    print(f\"   Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"   Precision: {precision:.4f}\")\n",
        "    print(f\"   Recall: {recall:.4f}\")\n",
        "    print(f\"   F1-Score: {f1:.4f}\")\n",
        "\n",
        "    # Confusion Matrix\n",
        "    print(f\"\\n📊 {model_name} Confusion Matrix:\")\n",
        "    confusion_matrix = predictions.groupBy(\"label\", \"prediction\").count().orderBy(\"label\", \"prediction\")\n",
        "    confusion_matrix.show()\n",
        "\n",
        "    # Detaylı sınıf bazlı metrikler\n",
        "    print(f\"\\n📊 {model_name} Sınıf Bazlı Metrikler:\")\n",
        "    for label_val in [0, 1]:\n",
        "        label_name = \"Yalan Haber\" if label_val == 0 else \"Doğru Haber\"\n",
        "\n",
        "        tp = predictions.filter((col(\"label\") == label_val) & (col(\"prediction\") == label_val)).count()\n",
        "        fp = predictions.filter((col(\"label\") != label_val) & (col(\"prediction\") == label_val)).count()\n",
        "        fn = predictions.filter((col(\"label\") == label_val) & (col(\"prediction\") != label_val)).count()\n",
        "        tn = predictions.filter((col(\"label\") != label_val) & (col(\"prediction\") != label_val)).count()\n",
        "\n",
        "        precision_class = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        recall_class = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        f1_class = 2 * (precision_class * recall_class) / (precision_class + recall_class) if (precision_class + recall_class) > 0 else 0\n",
        "\n",
        "        print(f\"   {label_name}:\")\n",
        "        print(f\"      Precision: {precision_class:.4f}\")\n",
        "        print(f\"      Recall: {recall_class:.4f}\")\n",
        "        print(f\"      F1-Score: {f1_class:.4f}\")\n",
        "\n",
        "    return {\n",
        "        \"model_name\": model_name,\n",
        "        \"auc\": auc,\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1\n",
        "    }\n"
      ],
      "metadata": {
        "id": "GoS23IR_ZOBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# 7. MODEL KAYDETME VE YÜKLEME\n",
        "# ========================\n",
        "def save_complete_pipeline(preprocessing_model, ml_model, model_path):\n",
        "    \"\"\"Tüm pipeline'ı kaydeder\"\"\"\n",
        "\n",
        "    import os\n",
        "    os.makedirs(model_path, exist_ok=True)\n",
        "\n",
        "    # Preprocessing pipeline'ı kaydet\n",
        "    preprocessing_path = f\"{model_path}/preprocessing_pipeline\"\n",
        "    preprocessing_model.write().overwrite().save(preprocessing_path)\n",
        "\n",
        "    # ML modelini kaydet\n",
        "    ml_model_path = f\"{model_path}/ml_model\"\n",
        "    ml_model.write().overwrite().save(ml_model_path)\n",
        "\n",
        "    print(f\"✅ Tüm pipeline kaydedildi: {model_path}\")\n",
        "\n",
        "def load_complete_pipeline(spark, model_path):\n",
        "    \"\"\"Kaydedilmiş pipeline'ı yükler\"\"\"\n",
        "    from pyspark.ml import PipelineModel\n",
        "    from pyspark.ml.classification import LogisticRegressionModel, NaiveBayesModel\n",
        "\n",
        "    preprocessing_path = f\"{model_path}/preprocessing_pipeline\"\n",
        "    ml_model_path = f\"{model_path}/ml_model\"\n",
        "\n",
        "    # Pipeline'ı yükle\n",
        "    preprocessing_pipeline = PipelineModel.load(preprocessing_path)\n",
        "\n",
        "    # ML modelini yükle\n",
        "    try:\n",
        "        ml_model = LogisticRegressionModel.load(ml_model_path)\n",
        "        model_type = \"LogisticRegression\"\n",
        "    except:\n",
        "        try:\n",
        "            ml_model = NaiveBayesModel.load(ml_model_path)\n",
        "            model_type = \"NaiveBayes\"\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"Model yüklenemedi: {e}\")\n",
        "\n",
        "    print(f\"✅ Pipeline yüklendi: {model_type}\")\n",
        "    return preprocessing_pipeline, ml_model, model_type\n"
      ],
      "metadata": {
        "id": "pZ3IqUNeZR33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# 8. YENİ METİN TAHMİNİ\n",
        "# ========================\n",
        "def predict_single_text(spark, preprocessing_pipeline, ml_model, text):\n",
        "    \"\"\"Tek bir metin için tahmin yapar\"\"\"\n",
        "\n",
        "    # DataFrame oluştur\n",
        "    new_data = spark.createDataFrame([(text,)], [\"text\"])\n",
        "\n",
        "    # Önişleme uygula\n",
        "    processed_data = preprocessing_pipeline.transform(new_data)\n",
        "\n",
        "    # Tahmin yap\n",
        "    prediction = ml_model.transform(processed_data)\n",
        "\n",
        "    # Sonucu al\n",
        "    result = prediction.select(\"text\", \"prediction\", \"probability\").collect()[0]\n",
        "\n",
        "    pred_label = int(result.prediction)\n",
        "    probability = result.probability.toArray()\n",
        "    confidence = max(probability)\n",
        "\n",
        "    news_type = \"DOĞRU HABER\" if pred_label == 1 else \"YALAN HABER\"\n",
        "\n",
        "    print(f\"\\n🔍 Tahmin Sonucu:\")\n",
        "    print(f\"   Metin: {text[:150]}...\")\n",
        "    print(f\"   Sonuç: {news_type}\")\n",
        "    print(f\"   Güven: {confidence:.4f}\")\n",
        "    print(f\"   Olasılıklar:\")\n",
        "    print(f\"      Yalan Haber: {probability[0]:.4f}\")\n",
        "    print(f\"      Doğru Haber: {probability[1]:.4f}\")\n",
        "\n",
        "    return pred_label, confidence\n"
      ],
      "metadata": {
        "id": "M_wSJiB2ZUR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ========================\n",
        "# 9. BATCH TAHMİN\n",
        "# ========================\n",
        "def predict_batch_texts(spark, preprocessing_pipeline, ml_model, texts):\n",
        "    \"\"\"Birden fazla metin için toplu tahmin\"\"\"\n",
        "\n",
        "    # DataFrame oluştur\n",
        "    texts_df = spark.createDataFrame([(text,) for text in texts], [\"text\"])\n",
        "\n",
        "    # Önişleme uygula\n",
        "    processed_df = preprocessing_pipeline.transform(texts_df)\n",
        "\n",
        "    # Tahmin yap\n",
        "    predictions_df = ml_model.transform(processed_df)\n",
        "\n",
        "    # Sonuçları al\n",
        "    results = predictions_df.select(\"text\", \"prediction\", \"probability\").collect()\n",
        "\n",
        "    print(f\"\\n🔍 Toplu Tahmin Sonuçları ({len(results)} metin):\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    batch_results = []\n",
        "    for i, result in enumerate(results, 1):\n",
        "        pred_label = int(result.prediction)\n",
        "        probability = result.probability.toArray()\n",
        "        confidence = max(probability)\n",
        "        news_type = \"DOĞRU\" if pred_label == 1 else \"YALAN\"\n",
        "\n",
        "        print(f\"{i}. Metin: {result.text[:100]}...\")\n",
        "        print(f\"   Sonuç: {news_type} HABER (Güven: {confidence:.4f})\")\n",
        "\n",
        "        batch_results.append({\n",
        "            \"text\": result.text,\n",
        "            \"prediction\": pred_label,\n",
        "            \"confidence\": confidence,\n",
        "            \"probabilities\": probability\n",
        "        })\n",
        "\n",
        "    return batch_results\n"
      ],
      "metadata": {
        "id": "GVpKHEg0ZWj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ========================\n",
        "# 10. ANA FONKSİYON\n",
        "# ========================\n",
        "def main():\n",
        "    \"\"\"Ana çalıştırma fonksiyonu\"\"\"\n",
        "\n",
        "    # Spark başlat\n",
        "    spark = initialize_spark()\n",
        "\n",
        "    try:\n",
        "        # 1. CSV veri yükleme\n",
        "        csv_path = \"etiketli_haberler.csv\"  # CSV dosyasının yolu\n",
        "        df = load_csv_data(spark, csv_path)\n",
        "\n",
        "        # 2. Veri keşfi\n",
        "        explore_data(df)\n",
        "\n",
        "        # 3. Train-Test split\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"📊 VERİ BÖLME\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "        train_count = train_df.count()\n",
        "        test_count = test_df.count()\n",
        "\n",
        "        print(f\"📊 Eğitim seti: {train_count}\")\n",
        "        print(f\"📊 Test seti: {test_count}\")\n",
        "\n",
        "        # Eğitim setinde etiket dağılımı\n",
        "        print(\"\\n📊 Eğitim Seti Etiket Dağılımı:\")\n",
        "        train_df.groupBy(\"label\").count().orderBy(\"label\").show()\n",
        "\n",
        "        # 4. Önişleme pipeline'ı\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"🔧 ÖNİŞLEME PİPELİNE\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        preprocessing_pipeline = create_preprocessing_pipeline()\n",
        "        preprocessing_model = preprocessing_pipeline.fit(train_df)\n",
        "\n",
        "        # Önişlemeyi uygula\n",
        "        train_processed = preprocessing_model.transform(train_df).cache()\n",
        "        test_processed = preprocessing_model.transform(test_df).cache()\n",
        "\n",
        "        print(\"✅ Önişleme tamamlandı\")\n",
        "\n",
        "        # 5. Model eğitimi ve karşılaştırma\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"🎯 MODEL EĞİTİMİ VE KARŞILAŞTIRMA\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        # Logistic Regression\n",
        "        lr_model = train_logistic_regression(train_processed)\n",
        "        lr_results = evaluate_model(lr_model, test_processed, \"Logistic Regression\")\n",
        "\n",
        "        # Naive Bayes\n",
        "        nb_model = train_naive_bayes(train_processed)\n",
        "        nb_results = evaluate_model(nb_model, test_processed, \"Naive Bayes\")\n",
        "\n",
        "        # 6. En iyi modeli seç\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"🏆 MODEL KARŞILAŞTIRMA\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        models_comparison = [lr_results, nb_results]\n",
        "        best_result = max(models_comparison, key=lambda x: x[\"f1\"])\n",
        "        best_model = lr_model if best_result[\"model_name\"] == \"Logistic Regression\" else nb_model\n",
        "\n",
        "        print(f\"🏆 En iyi model: {best_result['model_name']}\")\n",
        "        print(f\"   F1-Score: {best_result['f1']:.4f}\")\n",
        "        print(f\"   Accuracy: {best_result['accuracy']:.4f}\")\n",
        "        print(f\"   AUC-ROC: {best_result['auc']:.4f}\")\n",
        "\n",
        "        # 7. Model kaydetme\n",
        "        import traceback\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"💾 MODEL KAYDETME\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        model_save_path = \"./fake_news_model\"\n",
        "\n",
        "        try:\n",
        "            save_complete_pipeline(preprocessing_model, best_model, model_save_path)\n",
        "        except Exception as e:\n",
        "            print(\"❌ Hata oluştu:\")\n",
        "            traceback.print_exc()  # Hata detaylarını tam gösterir\n",
        "\n",
        "        # 8. Kaydedilmiş modeli test et\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"🔄 KAYDEDİLMİŞ MODEL TESTİ\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        loaded_preprocessing, loaded_model, model_type = load_complete_pipeline(spark, model_save_path)\n",
        "\n",
        "        # 9. Örnek tahminler\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"🧪 ÖRNEK TAHMİNLER\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        sample_texts = [\n",
        "            \"Cumhurbaşkanı bugün yeni ekonomik reformları açıkladı ve enflasyonla mücadele planını duyurdu.\",\n",
        "            \"Şok eden iddia: 5G kulelerinin koronavirüs yaydığı bilimsel olarak kanıtlandı!\",\n",
        "            \"Türkiye'nin ihracat rakamları geçen yıla göre yüzde 15 artış gösterdi.\",\n",
        "            \"İnanılmaz keşif: Bu bitki suyu içerek 1 ayda 20 kilo verebilirsiniz!\",\n",
        "            \"Merkez Bankası faiz oranlarını değiştirmeme kararı aldı.\",\n",
        "            \"Uzmanlar uyarıyor: Cep telefonu kullanımı beyin kanserine neden oluyor!\"\n",
        "        ]\n",
        "\n",
        "        # Tekli tahminler\n",
        "        for i, text in enumerate(sample_texts, 1):\n",
        "            print(f\"\\n--- Örnek {i} ---\")\n",
        "            predict_single_text(spark, loaded_preprocessing, loaded_model, text)\n",
        "\n",
        "        # Toplu tahmin\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"📊 TOPLU TAHMİN TESTİ\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        batch_results = predict_batch_texts(spark, loaded_preprocessing, loaded_model, sample_texts[:3])\n",
        "\n",
        "        print(\"\\n✅ Tüm işlemler başarıyla tamamlandı!\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Hata: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "\n",
        "    finally:\n",
        "        # Spark'ı kapat\n",
        "        spark.stop()\n",
        "        print(\"\\n🔄 Spark session sonlandırıldı\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQXhNfjTZY1F",
        "outputId": "8db93c84-0077-4b53-9176-adeb731f7ce3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Spark Session başlatıldı\n",
            "📋 Spark Version: 3.5.1\n",
            "📖 CSV dosyası okunuyor: etiketli_haberler.csv\n",
            "📋 Bulunan kolonlar: ['metin', 'etiket']\n",
            "📝 Metin kolonu: metin\n",
            "🏷️ Etiket kolonu: etiket\n",
            "📊 Unique etiketler: [1, 0]\n",
            "🔍 Örnek etiket değerleri: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "📊 Etiketler zaten sayısal\n",
            "✅ Toplam 4458 haber yüklendi\n",
            "   - Doğru Haber: <built-in method count of Row object at 0x7af8680efdd0>\n",
            "   - Yalan Haber: <built-in method count of Row object at 0x7af8680ec180>\n",
            "\n",
            "==================================================\n",
            "📊 VERİ KEŞFİ VE ANALİZİ\n",
            "==================================================\n",
            "📋 Temel İstatistikler:\n",
            "+-------+--------------------+-------------------+\n",
            "|summary|                text|              label|\n",
            "+-------+--------------------+-------------------+\n",
            "|  count|                4458|               4458|\n",
            "|   mean|                NULL| 0.5148048452220727|\n",
            "| stddev|                NULL|0.49983683229829845|\n",
            "|    min|a haber kar yağış...|                  0|\n",
            "|    max|şırnaktan acı hab...|                  1|\n",
            "+-------+--------------------+-------------------+\n",
            "\n",
            "📊 Etiket Dağılımı:\n",
            "+-----+-----+\n",
            "|label|count|\n",
            "+-----+-----+\n",
            "|    0| 2163|\n",
            "|    1| 2295|\n",
            "+-----+-----+\n",
            "\n",
            "📏 Metin Uzunluk İstatistikleri:\n",
            "+-------+------------------+\n",
            "|summary|       text_length|\n",
            "+-------+------------------+\n",
            "|  count|              4458|\n",
            "|   mean| 2114.125616868551|\n",
            "| stddev|1571.3156224018376|\n",
            "|    min|                17|\n",
            "|    max|             32584|\n",
            "+-------+------------------+\n",
            "\n",
            "📝 Örnek Metinler:\n",
            "\n",
            "1. Doğru Haber:\n",
            "   türkiyenin en büyüğü ve kapaklar açıldı güneydoğu anadolu projesi gap kapsamında inşa edilen dünyanın beşinci türkiyenin de en uzun sulama tüneli olan suruç tüneli ile yaklaşık yıl önce sulanmaya başl...\n",
            "\n",
            "2. Doğru Haber:\n",
            "   krem şanti nasıl yapılır doğal ve katkısız krem şanti tarifleri krem şanti pastaların ve tatlıların olmazsa olmaz malzemelerinden marketten alınan toz krem şantiler çok kısa süre içinde tüketime hazır...\n",
            "\n",
            "3. Doğru Haber:\n",
            "   yanlış anlaşılan mesaj vatandaşları sokağa döktü kütahya da doğalgaz dağıtımı yapan yetkili firma tarafından gönderilen borcunuz var mesajı üzerine çok sayıda vatandaş firma binası önünde toplandı...\n",
            "🔍 Null Değer Kontrolü:\n",
            "   Text null: 0\n",
            "   Label null: 0\n",
            "\n",
            "==================================================\n",
            "📊 VERİ BÖLME\n",
            "==================================================\n",
            "📊 Eğitim seti: 3610\n",
            "📊 Test seti: 848\n",
            "\n",
            "📊 Eğitim Seti Etiket Dağılımı:\n",
            "+-----+-----+\n",
            "|label|count|\n",
            "+-----+-----+\n",
            "|    0| 1734|\n",
            "|    1| 1876|\n",
            "+-----+-----+\n",
            "\n",
            "\n",
            "==================================================\n",
            "🔧 ÖNİŞLEME PİPELİNE\n",
            "==================================================\n",
            "🔧 Metin önişleme pipeline'ı oluşturuluyor...\n",
            "✅ Metin önişleme pipeline'ı oluşturuldu\n",
            "✅ Önişleme tamamlandı\n",
            "\n",
            "==================================================\n",
            "🎯 MODEL EĞİTİMİ VE KARŞILAŞTIRMA\n",
            "==================================================\n",
            "🔄 Logistic Regression eğitiliyor...\n",
            "✅ Logistic Regression eğitimi tamamlandı\n",
            "\n",
            "🔍 Logistic Regression modeli değerlendiriliyor...\n",
            "\n",
            "📊 Logistic Regression Model Sonuçları:\n",
            "   AUC-ROC: 0.9565\n",
            "   Accuracy: 0.8667\n",
            "   Precision: 0.8839\n",
            "   Recall: 0.8667\n",
            "   F1-Score: 0.8654\n",
            "\n",
            "📊 Logistic Regression Confusion Matrix:\n",
            "+-----+----------+-----+\n",
            "|label|prediction|count|\n",
            "+-----+----------+-----+\n",
            "|    0|       0.0|  328|\n",
            "|    0|       1.0|  101|\n",
            "|    1|       0.0|   12|\n",
            "|    1|       1.0|  407|\n",
            "+-----+----------+-----+\n",
            "\n",
            "\n",
            "📊 Logistic Regression Sınıf Bazlı Metrikler:\n",
            "   Yalan Haber:\n",
            "      Precision: 0.9647\n",
            "      Recall: 0.7646\n",
            "      F1-Score: 0.8531\n",
            "   Doğru Haber:\n",
            "      Precision: 0.8012\n",
            "      Recall: 0.9714\n",
            "      F1-Score: 0.8781\n",
            "🔄 Naive Bayes eğitiliyor...\n",
            "✅ Naive Bayes eğitimi tamamlandı\n",
            "\n",
            "🔍 Naive Bayes modeli değerlendiriliyor...\n",
            "\n",
            "📊 Naive Bayes Model Sonuçları:\n",
            "   AUC-ROC: 0.8058\n",
            "   Accuracy: 0.9222\n",
            "   Precision: 0.9296\n",
            "   Recall: 0.9222\n",
            "   F1-Score: 0.9218\n",
            "\n",
            "📊 Naive Bayes Confusion Matrix:\n",
            "+-----+----------+-----+\n",
            "|label|prediction|count|\n",
            "+-----+----------+-----+\n",
            "|    0|       0.0|  424|\n",
            "|    0|       1.0|    5|\n",
            "|    1|       0.0|   61|\n",
            "|    1|       1.0|  358|\n",
            "+-----+----------+-----+\n",
            "\n",
            "\n",
            "📊 Naive Bayes Sınıf Bazlı Metrikler:\n",
            "   Yalan Haber:\n",
            "      Precision: 0.8742\n",
            "      Recall: 0.9883\n",
            "      F1-Score: 0.9278\n",
            "   Doğru Haber:\n",
            "      Precision: 0.9862\n",
            "      Recall: 0.8544\n",
            "      F1-Score: 0.9156\n",
            "\n",
            "==================================================\n",
            "🏆 MODEL KARŞILAŞTIRMA\n",
            "==================================================\n",
            "🏆 En iyi model: Naive Bayes\n",
            "   F1-Score: 0.9218\n",
            "   Accuracy: 0.9222\n",
            "   AUC-ROC: 0.8058\n",
            "\n",
            "==================================================\n",
            "💾 MODEL KAYDETME\n",
            "==================================================\n",
            "✅ Tüm pipeline kaydedildi: ./fake_news_model\n",
            "\n",
            "==================================================\n",
            "🔄 KAYDEDİLMİŞ MODEL TESTİ\n",
            "==================================================\n",
            "✅ Pipeline yüklendi: NaiveBayes\n",
            "\n",
            "==================================================\n",
            "🧪 ÖRNEK TAHMİNLER\n",
            "==================================================\n",
            "\n",
            "--- Örnek 1 ---\n",
            "📢 Tahmin: Gerçek Haber ✅\n",
            "\n",
            "--- Örnek 2 ---\n",
            "📢 Tahmin: Yalan Haber ❌\n",
            "\n",
            "--- Örnek 3 ---\n",
            "📢 Tahmin: Gerçek Haber ✅\n",
            "\n",
            "--- Örnek 4 ---\n",
            "📢 Tahmin: Yalan Haber ❌\n",
            "\n",
            "--- Örnek 5 ---\n",
            "📢 Tahmin: Gerçek Haber ✅\n",
            "\n",
            "--- Örnek 6 ---\n",
            "📢 Tahmin: Yalan Haber ❌\n",
            "\n",
            "==================================================\n",
            "📊 TOPLU TAHMİN TESTİ\n",
            "==================================================\n",
            "\n",
            "🔍 Toplu Tahmin Sonuçları (3 metin):\n",
            "--------------------------------------------------------------------------------\n",
            "1. Metin: Cumhurbaşkanı bugün yeni ekonomik reformları açıkladı ve enflasyonla mücadele planını duyurdu....\n",
            "   Sonuç: DOĞRU HABER (Güven: 0.9453)\n",
            "2. Metin: Şok eden iddia: 5G kulelerinin koronavirüs yaydığı bilimsel olarak kanıtlandı!...\n",
            "   Sonuç: YALAN HABER (Güven: 1.0000)\n",
            "3. Metin: Türkiye'nin ihracat rakamları geçen yıla göre yüzde 15 artış gösterdi....\n",
            "   Sonuç: DOĞRU HABER (Güven: 0.9987)\n",
            "\n",
            "✅ Tüm işlemler başarıyla tamamlandı!\n",
            "\n",
            "🔄 Spark session sonlandırıldı\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r fake_news_model.zip fake_news_model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SNqxFqedhrR",
        "outputId": "041ade69-3e91-4c78-9f79-d0761feaaea0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: fake_news_model/ (stored 0%)\n",
            "  adding: fake_news_model/ml_model/ (stored 0%)\n",
            "  adding: fake_news_model/ml_model/metadata/ (stored 0%)\n",
            "  adding: fake_news_model/ml_model/metadata/part-00000 (deflated 48%)\n",
            "  adding: fake_news_model/ml_model/metadata/_SUCCESS (stored 0%)\n",
            "  adding: fake_news_model/ml_model/metadata/.part-00000.crc (stored 0%)\n",
            "  adding: fake_news_model/ml_model/metadata/._SUCCESS.crc (stored 0%)\n",
            "  adding: fake_news_model/ml_model/data/ (stored 0%)\n",
            "  adding: fake_news_model/ml_model/data/part-00000-1fdc2f49-5e3e-4a87-aec2-a105ca894ca5-c000.snappy.parquet (deflated 9%)\n",
            "  adding: fake_news_model/ml_model/data/.part-00000-1fdc2f49-5e3e-4a87-aec2-a105ca894ca5-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: fake_news_model/ml_model/data/_SUCCESS (stored 0%)\n",
            "  adding: fake_news_model/ml_model/data/._SUCCESS.crc (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/ (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/ (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/2_HashingTF_4fb752621ebf/ (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/2_HashingTF_4fb752621ebf/metadata/ (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/2_HashingTF_4fb752621ebf/metadata/part-00000 (deflated 37%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/2_HashingTF_4fb752621ebf/metadata/_SUCCESS (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/2_HashingTF_4fb752621ebf/metadata/.part-00000.crc (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/2_HashingTF_4fb752621ebf/metadata/._SUCCESS.crc (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/1_StopWordsRemover_9e616728d4ca/ (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/1_StopWordsRemover_9e616728d4ca/metadata/ (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/1_StopWordsRemover_9e616728d4ca/metadata/part-00000 (deflated 61%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/1_StopWordsRemover_9e616728d4ca/metadata/_SUCCESS (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/1_StopWordsRemover_9e616728d4ca/metadata/.part-00000.crc (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/1_StopWordsRemover_9e616728d4ca/metadata/._SUCCESS.crc (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/3_IDF_eefdce583c81/ (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/3_IDF_eefdce583c81/metadata/ (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/3_IDF_eefdce583c81/metadata/part-00000 (deflated 35%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/3_IDF_eefdce583c81/metadata/_SUCCESS (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/3_IDF_eefdce583c81/metadata/.part-00000.crc (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/3_IDF_eefdce583c81/metadata/._SUCCESS.crc (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/3_IDF_eefdce583c81/data/ (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/3_IDF_eefdce583c81/data/part-00000-e21e12bc-0711-4259-8d0c-1a0717c11ba3-c000.snappy.parquet (deflated 50%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/3_IDF_eefdce583c81/data/.part-00000-e21e12bc-0711-4259-8d0c-1a0717c11ba3-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/3_IDF_eefdce583c81/data/_SUCCESS (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/3_IDF_eefdce583c81/data/._SUCCESS.crc (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/0_Tokenizer_ea73d0c7c743/ (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/0_Tokenizer_ea73d0c7c743/metadata/ (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/0_Tokenizer_ea73d0c7c743/metadata/part-00000 (deflated 34%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/0_Tokenizer_ea73d0c7c743/metadata/_SUCCESS (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/0_Tokenizer_ea73d0c7c743/metadata/.part-00000.crc (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/stages/0_Tokenizer_ea73d0c7c743/metadata/._SUCCESS.crc (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/metadata/ (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/metadata/part-00000 (deflated 22%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/metadata/_SUCCESS (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/metadata/.part-00000.crc (stored 0%)\n",
            "  adding: fake_news_model/preprocessing_pipeline/metadata/._SUCCESS.crc (stored 0%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"fake_news_model.zip\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "QXBd3DYXdpGS",
        "outputId": "545471c3-0e6a-431e-c39f-054f1a7301d1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_1d71e369-1f55-4e15-b100-ccb332c87ef7\", \"fake_news_model.zip\", 166245)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}